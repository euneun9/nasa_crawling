import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import pdfkit
import os

config = pdfkit.configuration(wkhtmltopdf='C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe')  # ê²½ë¡œ ì„¤ì • ì•ˆ í•´ë„ ë˜ë©´ None

# 1. frameset í˜ì´ì§€ ìš”ì²­
base_url = "https://workmanship.nasa.gov/lib/insp/2%20books/frameset.html"
response = requests.get(base_url)
soup = BeautifulSoup(response.text, 'html.parser')

# 2. <frame> src ì¶”ì¶œ
frame_srcs = [frame.get('src') for frame in soup.find_all('frame') if frame.get('src')]

# 3. ëª¨ë“  í”„ë ˆì„ì—ì„œ href ìˆ˜ì§‘
all_links = []
for frame_src in frame_srcs:
    frame_url = urljoin(base_url, frame_src)
    try:
        frame_response = requests.get(frame_url)
        frame_soup = BeautifulSoup(frame_response.text, 'html.parser')
        for a_tag in frame_soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urljoin(frame_url, href)
            all_links.append(full_url)
    except Exception as e:
        print(f"âŒ Failed to process {frame_url}: {e}")

# ì¤‘ë³µ ì œê±°
all_links = list(set(all_links))

# 4. ì €ì¥ í´ë” ìƒì„±
os.makedirs("nasa_pdfs", exist_ok=True)

# 5. ê° ë§í¬ë¥¼ PDFë¡œ ì €ì¥
for link in all_links:
    try:
        filename = link.rstrip('/').split('/')[-1]
        if not filename.endswith('.pdf'):
            filename += '.pdf'
        save_path = os.path.join("nasa_pdfs", filename)
        print(f"ğŸ“„ Saving {link} â†’ {save_path}")
        pdfkit.from_url(link, save_path, configuration=config)
    except Exception as e:
        print(f"âŒ Error saving {link}: {e}")

print("âœ… All done.")
